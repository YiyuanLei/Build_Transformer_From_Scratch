# Terminologies
### Input embedding
- It contains semantic information from the token embedding
- Embedding matrix $V\times d$ is generated by converting input tokens (vocabulary size) of $V$ into a vector of d dimension;
- Given a word $i$ from the input token, it maps to $E_{i,:}$, which is the $i^{th}$ row of the matrix.
### Position embedding [link](https://medium.com/autonomous-agents/math-behind-positional-embeddings-in-transformer-models-921db18b0c28)
- Provide model the sequence of the input tokens
- $PE_{(pos, 2i)} = sin(\frac{pos}{1000^{\frac{2i}{d}}})$
- $PE_{(pos, 2i+1)} = sin(\frac{pos}{1000^{\frac{2i}{d}}})$
- pos: position of the token sequence
- i is the ith dimension of the embedding vector
- d is the dimensionality of the embedding
- Why $1000^{^{\frac{2i}{d}}}$?
  1) a wider control of the wavelength (handling larger sequence); 
  2) stable derivatives
  3) No additional parameters to learn
- Why do we use sinusoidal function?
  1) Extrapolation for positional relationships that not encountered before
  2) stable derivatives